{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bc44e8",
   "metadata": {
    "time_run": "7:30:12p"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to 2.tcp.ngrok.io:19261 as solvituser...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Connection successful!\n",
      "\n",
      "âœ“ VMS Magic command ready:\n",
      "  - %%vms                                      : Execute shell commands\n",
      "  - %%vms python                               : Execute Python (auto venv)\n",
      "  - %%vms python:venv_name                     : Execute Python (specific venv)\n",
      "  - %%vms python persistent file.py            : Persistent Python (auto venv)\n",
      "  - %%vms python:venv_name persistent file.py  : Persistent Python (specific venv)\n",
      "  - setup_venv(name, packages, force)          : Setup virtual environment\n"
     ]
    }
   ],
   "source": [
    "import paramiko\n",
    "from IPython.core.magic import register_cell_magic\n",
    "\n",
    "def load_connection_config(config_file='connection_config.txt'):\n",
    "    \"\"\"Load connection configuration from file\"\"\"\n",
    "    config = {}\n",
    "    with open(config_file, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if '=' in line and not line.startswith('#'):\n",
    "                key, value = line.split('=', 1)\n",
    "                key = key.strip()\n",
    "                value = value.strip()\n",
    "                # Convert port to integer\n",
    "                if key == 'port':\n",
    "                    value = int(value)\n",
    "                config[key] = value\n",
    "    return config\n",
    "\n",
    "# Load configuration\n",
    "try:\n",
    "    secrets = load_connection_config('connection_config.txt')\n",
    "    hostname = secrets['hostname']\n",
    "    port = secrets['port']\n",
    "    username = secrets['username']\n",
    "    password = secrets['password']\n",
    "    \n",
    "    # Test connection\n",
    "    print(f\"Connecting to {hostname}:{port} as {username}...\")\n",
    "    client = paramiko.SSHClient()\n",
    "    client.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "    client.connect(hostname=hostname, port=port, username=username, password=password, timeout=10)\n",
    "    print(\"âœ“ Connection successful!\\n\")\n",
    "    client.close()\n",
    "except FileNotFoundError:\n",
    "    print(\"âš  Warning: connection_config.txt not found. Please create it first.\")\n",
    "    hostname = port = username = password = None\n",
    "except Exception as e:\n",
    "    print(f\"âœ— Connection failed: {e}\\n\")\n",
    "    hostname = port = username = password = None\n",
    "\n",
    "# Unified VMS command\n",
    "@register_cell_magic\n",
    "def vms(line, cell):\n",
    "    \"\"\"\n",
    "    Unified VMS command - handles shell, Python, and file operations\n",
    "    \n",
    "    Usage:\n",
    "        %%vms                                      # Execute shell commands\n",
    "        %%vms python                               # Execute Python (uses venv if available)\n",
    "        %%vms python:ml_env                        # Execute Python with specific venv\n",
    "        %%vms python persistent script.py          # Append to file and execute\n",
    "        %%vms python:ml_env persistent script.py   # Append to file and execute in venv\n",
    "    \"\"\"\n",
    "    client = paramiko.SSHClient()\n",
    "    client.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "    client.connect(hostname=hostname, port=port, username=username, password=password, timeout=10)\n",
    "    \n",
    "    line = line.strip()\n",
    "    \n",
    "    # Mode 1: Shell commands (default)\n",
    "    if not line or not line.startswith('python'):\n",
    "        stdin, stdout, stderr = client.exec_command(cell)\n",
    "        output = stdout.read().decode()\n",
    "        errors = stderr.read().decode()\n",
    "        \n",
    "        client.close()\n",
    "        \n",
    "        if errors:\n",
    "            print(\"STDERR:\", errors)\n",
    "        if output:\n",
    "            print(output)\n",
    "        return\n",
    "    \n",
    "    # Parse Python modes\n",
    "    venv_name = None\n",
    "    persistent = False\n",
    "    filename = 'persistent.py'\n",
    "    \n",
    "    # Check if specific venv is specified (python:venv_name)\n",
    "    if ':' in line:\n",
    "        prefix, rest = line.split(':', 1)\n",
    "        rest_parts = rest.strip().split()\n",
    "        venv_name = rest_parts[0]\n",
    "        \n",
    "        # Check for persistent mode\n",
    "        if len(rest_parts) > 1 and rest_parts[1] == 'persistent':\n",
    "            persistent = True\n",
    "            if len(rest_parts) > 2:\n",
    "                filename = rest_parts[2]\n",
    "    else:\n",
    "        # No specific venv, check for persistent mode\n",
    "        parts = line.split()\n",
    "        if len(parts) > 1 and parts[1] == 'persistent':\n",
    "            persistent = True\n",
    "            if len(parts) > 2:\n",
    "                filename = parts[2]\n",
    "    \n",
    "    # Determine which Python to use\n",
    "    if venv_name:\n",
    "        # Use specified venv\n",
    "        python_cmd = f'{venv_name}/bin/python3'\n",
    "    else:\n",
    "        # Auto-detect default venv\n",
    "        default_venv = 'ml_env'\n",
    "        stdin, stdout, stderr = client.exec_command(f'test -f {default_venv}/bin/python3 && echo \"yes\" || echo \"no\"')\n",
    "        venv_exists = stdout.read().decode().strip() == \"yes\"\n",
    "        python_cmd = f'{default_venv}/bin/python3' if venv_exists else 'python3'\n",
    "    \n",
    "    # Mode 2 & 3: Python execution (non-persistent)\n",
    "    if not persistent:\n",
    "        command = f'{python_cmd} << EOF\\n{cell}\\nEOF'\n",
    "        stdin, stdout, stderr = client.exec_command(command)\n",
    "        output = stdout.read().decode()\n",
    "        errors = stderr.read().decode()\n",
    "        \n",
    "        client.close()\n",
    "        \n",
    "        if errors:\n",
    "            print(\"STDERR:\", errors)\n",
    "        if output:\n",
    "            print(output)\n",
    "        return\n",
    "    \n",
    "    # Mode 4 & 5: Persistent Python execution\n",
    "    command = f'cat >> {filename} << EOF\\n{cell}\\nEOF\\n{python_cmd} {filename}'\n",
    "    stdin, stdout, stderr = client.exec_command(command)\n",
    "    output = stdout.read().decode()\n",
    "    errors = stderr.read().decode()\n",
    "    \n",
    "    client.close()\n",
    "    \n",
    "    if errors:\n",
    "        print(\"STDERR:\", errors)\n",
    "    if output:\n",
    "        print(output)\n",
    "\n",
    "def setup_venv(venv_name='ml_env', packages=None, force_reinstall=False):\n",
    "    \"\"\"\n",
    "    Set up a Python virtual environment on remote machine with ML packages\n",
    "    \n",
    "    Args:\n",
    "        venv_name: Name of the virtual environment (default: 'ml_env')\n",
    "        packages: List of packages to install (default: numpy, pandas, matplotlib, scikit-learn, fastai, tinygrad)\n",
    "        force_reinstall: If True, removes existing venv and creates fresh one\n",
    "    \"\"\"\n",
    "    if packages is None:\n",
    "        packages = ['numpy', 'pandas', 'matplotlib', 'scikit-learn', 'fastai', 'tinygrad']\n",
    "    \n",
    "    client = paramiko.SSHClient()\n",
    "    client.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "    client.connect(hostname=hostname, port=port, username=username, password=password, timeout=10)\n",
    "    \n",
    "    print(f\"Setting up virtual environment: {venv_name}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Step 1: Remove existing venv if force_reinstall\n",
    "    if force_reinstall:\n",
    "        print(\"\\n1. Removing existing virtual environment...\")\n",
    "        stdin, stdout, stderr = client.exec_command(f'rm -rf {venv_name}')\n",
    "        stdout.channel.recv_exit_status()\n",
    "        print(\"   âœ“ Cleaned up old environment\")\n",
    "    \n",
    "    # Step 2: Check if venv exists\n",
    "    print(\"\\n2. Checking for existing virtual environment...\")\n",
    "    stdin, stdout, stderr = client.exec_command(f'test -d {venv_name} && echo \"exists\" || echo \"not found\"')\n",
    "    exists = stdout.read().decode().strip()\n",
    "    \n",
    "    if exists == \"not found\":\n",
    "        print(f\"   Creating new virtual environment: {venv_name}\")\n",
    "        stdin, stdout, stderr = client.exec_command(f'python3 -m venv {venv_name}')\n",
    "        stdout.channel.recv_exit_status()\n",
    "        print(\"   âœ“ Virtual environment created\")\n",
    "    else:\n",
    "        print(f\"   âœ“ Virtual environment already exists: {venv_name}\")\n",
    "    \n",
    "    # Step 3: Upgrade pip\n",
    "    print(\"\\n3. Upgrading pip...\")\n",
    "    stdin, stdout, stderr = client.exec_command(\n",
    "        f'{venv_name}/bin/pip install --upgrade pip'\n",
    "    )\n",
    "    stdout.channel.recv_exit_status()\n",
    "    print(\"   âœ“ Pip upgraded\")\n",
    "    \n",
    "    # Step 4: Install packages\n",
    "    print(\"\\n4. Installing packages...\")\n",
    "    packages_str = ' '.join(packages)\n",
    "    print(f\"   Installing: {packages_str}\")\n",
    "    \n",
    "    stdin, stdout, stderr = client.exec_command(\n",
    "        f'{venv_name}/bin/pip install {packages_str}'\n",
    "    )\n",
    "    \n",
    "    # Stream output\n",
    "    while True:\n",
    "        line = stdout.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        print(f\"   {line.rstrip()}\")\n",
    "    \n",
    "    stdout.channel.recv_exit_status()\n",
    "    \n",
    "    # Step 5: Verify installation\n",
    "    print(\"\\n5. Verifying installation...\")\n",
    "    package_pattern = '|'.join(packages)\n",
    "    stdin, stdout, stderr = client.exec_command(\n",
    "        f'{venv_name}/bin/pip list | grep -E \"{package_pattern}\"'\n",
    "    )\n",
    "    installed = stdout.read().decode()\n",
    "    print(f\"\\n   Installed packages:\\n{installed}\")\n",
    "    \n",
    "    client.close()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"âœ“ Virtual environment setup complete!\")\n",
    "    print(f\"\\nUsage:\")\n",
    "    print(f\"   %%vms python:{venv_name}\")\n",
    "    print(f\"   %%vms python:{venv_name} persistent script.py\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "if hostname:\n",
    "    print(\"âœ“ VMS Magic command ready:\")\n",
    "    print(\"  - %%vms                                      : Execute shell commands\")\n",
    "    print(\"  - %%vms python                               : Execute Python (auto venv)\")\n",
    "    print(\"  - %%vms python:venv_name                     : Execute Python (specific venv)\")\n",
    "    print(\"  - %%vms python persistent file.py            : Persistent Python (auto venv)\")\n",
    "    print(\"  - %%vms python:venv_name persistent file.py  : Persistent Python (specific venv)\")\n",
    "    print(\"  - setup_venv(name, packages, force)          : Setup virtual environment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8890597c",
   "metadata": {},
   "source": [
    "### Testing tinygrad with my local Intel GPU\n",
    "\n",
    "This demonstrates training a neural network using tinygrad on a remote machine with Intel GPU. Pytorch does not work with Intel so \n",
    "Tinygrad was chosen to demonstrate:\n",
    "\n",
    "**Steps:**\n",
    "1. **Setup environment** - Create a virtual environment with tinygrad and dependencies (numpy, pillow, requests, tqdm)\n",
    "2. **Download dataset** - Fetch MNIST handwritten digits dataset from fastai's collection (60k training images, 10k test images)\n",
    "3. **Build model** - Create a simple 2-layer neural network (784 input â†’ 128 hidden â†’ 10 output) using tinygrad's API\n",
    "4. **Train model** - Run training for 5 epochs with SGD optimizer, batch size 128, learning rate 0.01\n",
    "5. **Monitor results** - Track loss per epoch, validation accuracy during training, and final test accuracy\n",
    "6. **Check GPU usage** - Verify Intel GPU detection and compute runtime availability\n",
    "\n",
    "**Key features demonstrated:**\n",
    "- Remote Python execution with virtual environments using `%%vms python:venv_name persistent file.py`\n",
    "- Tinygrad's tensor operations, automatic differentiation, and optimizer with Intel GPU \n",
    "- Training loop with batching, forward/backward passes, and metrics logging \n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30f15b1",
   "metadata": {
    "time_run": "8:57:41p"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up virtual environment: tinygrad_env\n",
      "============================================================\n",
      "\n",
      "2. Checking for existing virtual environment...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ“ Virtual environment already exists: tinygrad_env\n",
      "\n",
      "3. Upgrading pip...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ“ Pip upgraded\n",
      "\n",
      "4. Installing packages...\n",
      "   Installing: tinygrad numpy pillow requests tqdm kagglehub\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Requirement already satisfied: tinygrad in ./tinygrad_env/lib/python3.12/site-packages (0.11.0)\n",
      "   Requirement already satisfied: numpy in ./tinygrad_env/lib/python3.12/site-packages (2.3.4)\n",
      "   Requirement already satisfied: pillow in ./tinygrad_env/lib/python3.12/site-packages (12.0.0)\n",
      "   Requirement already satisfied: requests in ./tinygrad_env/lib/python3.12/site-packages (2.32.5)\n",
      "   Requirement already satisfied: tqdm in ./tinygrad_env/lib/python3.12/site-packages (4.67.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Collecting kagglehub\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Downloading kagglehub-0.3.13-py3-none-any.whl.metadata (38 kB)\n",
      "   Requirement already satisfied: charset_normalizer<4,>=2 in ./tinygrad_env/lib/python3.12/site-packages (from requests) (3.4.4)\n",
      "   Requirement already satisfied: idna<4,>=2.5 in ./tinygrad_env/lib/python3.12/site-packages (from requests) (3.11)\n",
      "   Requirement already satisfied: urllib3<3,>=1.21.1 in ./tinygrad_env/lib/python3.12/site-packages (from requests) (2.5.0)\n",
      "   Requirement already satisfied: certifi>=2017.4.17 in ./tinygrad_env/lib/python3.12/site-packages (from requests) (2025.10.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Collecting packaging (from kagglehub)\n",
      "     Using cached packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "   Collecting pyyaml (from kagglehub)\n",
      "     Using cached pyyaml-6.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.4 kB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Downloading kagglehub-0.3.13-py3-none-any.whl (68 kB)\n",
      "   Using cached packaging-25.0-py3-none-any.whl (66 kB)\n",
      "   Using cached pyyaml-6.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (807 kB)\n",
      "   Installing collected packages: pyyaml, packaging, kagglehub\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   \n",
      "   Successfully installed kagglehub-0.3.13 packaging-25.0 pyyaml-6.0.3\n",
      "\n",
      "5. Verifying installation...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   Installed packages:\n",
      "kagglehub                0.3.13\n",
      "numpy                    2.3.4\n",
      "pillow                   12.0.0\n",
      "requests                 2.32.5\n",
      "tinygrad                 0.11.0\n",
      "tqdm                     4.67.1\n",
      "\n",
      "\n",
      "============================================================\n",
      "âœ“ Virtual environment setup complete!\n",
      "\n",
      "Usage:\n",
      "   %%vms python:tinygrad_env\n",
      "   %%vms python:tinygrad_env persistent script.py\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Create venv with tinygrad and necessary packages\n",
    "setup_venv(\n",
    "    venv_name='tinygrad_env', \n",
    "    packages=['tinygrad', 'numpy', 'pillow', 'requests', 'tqdm', 'kagglehub'],\n",
    "    force_reinstall=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b772094b",
   "metadata": {
    "time_run": "8:58:51p"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDERR: \r",
      "  0%|          | 0.00/22.0M [00:00<?, ?B/s]\r",
      "  5%|â–         | 1.00M/22.0M [00:00<00:14, 1.56MB/s]\r",
      "  9%|â–‰         | 2.00M/22.0M [00:00<00:07, 2.97MB/s]\r",
      " 18%|â–ˆâ–Š        | 4.00M/22.0M [00:00<00:03, 6.03MB/s]\r",
      " 23%|â–ˆâ–ˆâ–Ž       | 5.00M/22.0M [00:01<00:03, 5.11MB/s]\r",
      " 27%|â–ˆâ–ˆâ–‹       | 6.00M/22.0M [00:01<00:03, 4.42MB/s]\r",
      " 32%|â–ˆâ–ˆâ–ˆâ–      | 7.00M/22.0M [00:01<00:03, 4.56MB/s]\r",
      " 36%|â–ˆâ–ˆâ–ˆâ–‹      | 8.00M/22.0M [00:01<00:03, 4.73MB/s]\r",
      " 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 9.00M/22.0M [00:02<00:03, 4.36MB/s]\r",
      " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 10.0M/22.0M [00:02<00:03, 3.36MB/s]\r",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 11.0M/22.0M [00:03<00:05, 1.98MB/s]\r",
      " 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 12.0M/22.0M [00:04<00:05, 1.86MB/s]\r",
      " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 13.0M/22.0M [00:04<00:04, 2.04MB/s]\r",
      " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 14.0M/22.0M [00:05<00:03, 2.38MB/s]\r",
      " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 15.0M/22.0M [00:05<00:02, 2.52MB/s]\r",
      " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 16.0M/22.0M [00:05<00:02, 3.03MB/s]\r",
      " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 17.0M/22.0M [00:05<00:01, 3.52MB/s]\r",
      " 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 18.0M/22.0M [00:05<00:01, 3.93MB/s]\r",
      " 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 19.0M/22.0M [00:06<00:00, 3.37MB/s]\r",
      " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 20.0M/22.0M [00:06<00:00, 4.20MB/s]\r",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 22.0M/22.0M [00:06<00:00, 5.88MB/s]\r",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22.0M/22.0M [00:06<00:00, 3.46MB/s]\n",
      "\n",
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/hojjatk/mnist-dataset?dataset_version_number=1...\n",
      "Extracting files...\n",
      "Path to dataset files: /home/solvituser/.cache/kagglehub/datasets/hojjatk/mnist-dataset/versions/1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%vms python:tinygrad_env \n",
    "\n",
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"hojjatk/mnist-dataset\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1ba772",
   "metadata": {
    "time_run": "9:06:45p"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded to cache: /home/solvituser/.cache/kagglehub/datasets/hojjatk/mnist-dataset/versions/1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%vms python:tinygrad_env \n",
    "\n",
    "import kagglehub\n",
    "# Download to cache\n",
    "cache_path = kagglehub.dataset_download(\"hojjatk/mnist-dataset\")\n",
    "print(\"Downloaded to cache:\", cache_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e3ecaf",
   "metadata": {
    "time_run": "9:07:25p"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 53M\n",
      "drwxrwxr-x 6 solvituser solvituser 4.0K Oct 15 22:58 .\n",
      "drwxrwxr-x 3 solvituser solvituser 4.0K Oct 15 22:58 ..\n",
      "drwxrwxr-x 2 solvituser solvituser 4.0K Oct 15 22:58 t10k-images-idx3-ubyte\n",
      "-rw-rw-r-- 1 solvituser solvituser 7.5M Oct 15 22:58 t10k-images.idx3-ubyte\n",
      "drwxrwxr-x 2 solvituser solvituser 4.0K Oct 15 22:58 t10k-labels-idx1-ubyte\n",
      "-rw-rw-r-- 1 solvituser solvituser 9.8K Oct 15 22:58 t10k-labels.idx1-ubyte\n",
      "drwxrwxr-x 2 solvituser solvituser 4.0K Oct 15 22:58 train-images-idx3-ubyte\n",
      "-rw-rw-r-- 1 solvituser solvituser  45M Oct 15 22:58 train-images.idx3-ubyte\n",
      "drwxrwxr-x 2 solvituser solvituser 4.0K Oct 15 22:58 train-labels-idx1-ubyte\n",
      "-rw-rw-r-- 1 solvituser solvituser  59K Oct 15 22:58 train-labels.idx1-ubyte\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%vms\n",
    "\n",
    "ls -lah /home/solvituser/.cache/kagglehub/datasets/hojjatk/mnist-dataset/versions/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53492a83",
   "metadata": {
    "time_run": "9:11:03p"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data...\n",
      "  Train images shape: (60000, 28, 28)\n",
      "  Train labels shape: (60000,)\n",
      "\n",
      "Loading test data...\n",
      "  Test images shape: (10000, 28, 28)\n",
      "  Test labels shape: (10000,)\n",
      "\n",
      "Saving data as numpy arrays...\n",
      "\n",
      "âœ“ Data loaded and saved successfully!\n",
      "\n",
      "Data summary:\n",
      "  Training samples: 60000\n",
      "  Test samples: 10000\n",
      "  Image size: 28x28\n",
      "  Number of classes: 10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%vms python:tinygrad_env persistent mnist_loader.py\n",
    "\n",
    "import struct\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def load_idx_images(filename):\n",
    "    \"\"\"Load images from IDX file format\"\"\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        # Read magic number and dimensions\n",
    "        magic = struct.unpack('>I', f.read(4))[0]\n",
    "        num_images = struct.unpack('>I', f.read(4))[0]\n",
    "        rows = struct.unpack('>I', f.read(4))[0]\n",
    "        cols = struct.unpack('>I', f.read(4))[0]\n",
    "        \n",
    "        # Read image data\n",
    "        images = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "        images = images.reshape(num_images, rows, cols)\n",
    "        \n",
    "        return images\n",
    "\n",
    "def load_idx_labels(filename):\n",
    "    \"\"\"Load labels from IDX file format\"\"\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        # Read magic number and count\n",
    "        magic = struct.unpack('>I', f.read(4))[0]\n",
    "        num_labels = struct.unpack('>I', f.read(4))[0]\n",
    "        \n",
    "        # Read label data\n",
    "        labels = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "        \n",
    "        return labels\n",
    "\n",
    "# Path to dataset\n",
    "data_path = Path('/home/solvituser/.cache/kagglehub/datasets/hojjatk/mnist-dataset/versions/1')\n",
    "\n",
    "# Load training data\n",
    "print(\"Loading training data...\")\n",
    "train_images = load_idx_images(data_path / 'train-images.idx3-ubyte')\n",
    "train_labels = load_idx_labels(data_path / 'train-labels.idx1-ubyte')\n",
    "print(f\"  Train images shape: {train_images.shape}\")\n",
    "print(f\"  Train labels shape: {train_labels.shape}\")\n",
    "\n",
    "# Load test data\n",
    "print(\"\\nLoading test data...\")\n",
    "test_images = load_idx_images(data_path / 't10k-images.idx3-ubyte')\n",
    "test_labels = load_idx_labels(data_path / 't10k-labels.idx1-ubyte')\n",
    "print(f\"  Test images shape: {test_images.shape}\")\n",
    "print(f\"  Test labels shape: {test_labels.shape}\")\n",
    "\n",
    "# Save as numpy arrays\n",
    "print(\"\\nSaving data as numpy arrays...\")\n",
    "np.save('train_images.npy', train_images)\n",
    "np.save('train_labels.npy', train_labels)\n",
    "np.save('test_images.npy', test_images)\n",
    "np.save('test_labels.npy', test_labels)\n",
    "\n",
    "print(\"\\nâœ“ Data loaded and saved successfully!\")\n",
    "print(\"\\nData summary:\")\n",
    "print(f\"  Training samples: {len(train_images)}\")\n",
    "print(f\"  Test samples: {len(test_images)}\")\n",
    "print(f\"  Image size: {train_images.shape[1]}x{train_images.shape[2]}\")\n",
    "print(f\"  Number of classes: {len(np.unique(train_labels))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b07cab3",
   "metadata": {
    "time_run": "9:11:14p"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current device: GPU\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%vms python:tinygrad_env\n",
    "\n",
    "from tinygrad import Device\n",
    "\n",
    "print(f\"Current device: {Device.DEFAULT}\") #make sure user has access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74e078b",
   "metadata": {
    "time_run": "9:11:20p"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDERR: Traceback (most recent call last):\n",
      "  File \"/home/solvituser/train_mnist.py\", line 17, in <module>\n",
      "    with gzip.open(data_path / 'mnist.pkl.gz', 'rb') as f:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/gzip.py\", line 61, in open\n",
      "    binary_file = GzipFile(filename, gz_mode, compresslevel)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/gzip.py\", line 192, in __init__\n",
      "    fileobj = self.myfileobj = builtins.open(filename, mode or 'rb')\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'mnist_data/mnist.pkl.gz'\n",
      "\n",
      "============================================================\n",
      "MNIST Training with Tinygrad on Intel GPU\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%vms python:tinygrad_env persistent train_mnist.py\n",
    "\n",
    "import pickle\n",
    "import gzip\n",
    "import numpy as np\n",
    "from tinygrad.tensor import Tensor\n",
    "from tinygrad.nn import Linear\n",
    "from tinygrad.nn.optim import SGD\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MNIST Training with Tinygrad on Intel GPU\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load data\n",
    "data_path = Path('mnist_data')\n",
    "with gzip.open(data_path / 'mnist.pkl.gz', 'rb') as f:\n",
    "    train_data, val_data, test_data = pickle.load(f, encoding='latin1')\n",
    "\n",
    "X_train, y_train = train_data\n",
    "X_val, y_val = val_data\n",
    "\n",
    "# Use smaller subset for quick demo\n",
    "n_samples = 5000\n",
    "X_train, y_train = X_train[:n_samples], y_train[:n_samples]\n",
    "X_val, y_val = X_val[:1000], y_val[:1000]\n",
    "\n",
    "print(f\"\\nDataset loaded:\")\n",
    "print(f\"  Training: {len(X_train)} samples\")\n",
    "print(f\"  Validation: {len(X_val)} samples\")\n",
    "\n",
    "# Simple 2-layer neural network\n",
    "class SimpleNet:\n",
    "    def __init__(self):\n",
    "        self.l1 = Linear(784, 128)\n",
    "        self.l2 = Linear(128, 10)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        x = self.l1(x).relu()\n",
    "        return self.l2(x)\n",
    "\n",
    "model = SimpleNet()\n",
    "opt = SGD([model.l1.weight, model.l1.bias, model.l2.weight, model.l2.bias], lr=0.01)\n",
    "\n",
    "print(f\"\\nModel architecture:\")\n",
    "print(f\"  Input: 784 -> Hidden: 128 -> Output: 10\")\n",
    "print(f\"  Optimizer: SGD (lr=0.01)\")\n",
    "\n",
    "# Training loop\n",
    "batch_size = 128\n",
    "epochs = 5\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Starting training...\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_start = time.time()\n",
    "    epoch_loss = 0\n",
    "    n_batches = 0\n",
    "    \n",
    "    # Shuffle data\n",
    "    indices = np.random.permutation(len(X_train))\n",
    "    Tensor.training=True    \n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        batch_idx = indices[i:i+batch_size]\n",
    "        X_batch = Tensor(X_train[batch_idx])\n",
    "        y_batch = Tensor(y_train[batch_idx])\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = model(X_batch)\n",
    "        \n",
    "        # Compute loss using one-hot encoding\n",
    "        y_onehot = Tensor(np.eye(10)[y_train[batch_idx]])\n",
    "        log_probs = logits.log_softmax(axis=1)\n",
    "        loss = -(y_onehot * log_probs).sum() / len(batch_idx)\n",
    "        \n",
    "        # Backward pass\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        epoch_loss += loss.numpy()\n",
    "        n_batches += 1\n",
    "    \n",
    "    # Validation accuracy\n",
    "    val_logits = model(Tensor(X_val))\n",
    "    val_preds = val_logits.numpy().argmax(axis=1)\n",
    "    val_acc = (val_preds == y_val).mean()\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start\n",
    "    avg_loss = epoch_loss / n_batches\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{epochs} | Loss: {avg_loss:.4f} | Val Acc: {val_acc*100:.2f}% | Time: {epoch_time:.2f}s\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Training complete!\")\n",
    "print(f\"{'='*60}\")\n",
    "Tensor.training=False\n",
    "# Final test accuracy\n",
    "test_logits = model(Tensor(test_data[0]))\n",
    "test_preds = test_logits.numpy().argmax(axis=1)\n",
    "test_acc = (test_preds == test_data[1]).mean()\n",
    "print(f\"\\nFinal Test Accuracy: {test_acc*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f67b199",
   "metadata": {
    "solveit_ai": true
   },
   "source": [
    "ok let's go through the error carefully to identify\n",
    "\n",
    "##### ðŸ¤–ReplyðŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "Looking at the error, the issue is clear:\n",
    "\n",
    "```\n",
    "FileNotFoundError: [Errno 2] No such file or directory: 'mnist_data/mnist.pkl.gz'\n",
    "```\n",
    "\n",
    "The script is trying to load data from `mnist_data/mnist.pkl.gz`, but this file doesn't exist. \n",
    "\n",
    "However, you already have the MNIST data! Look back at your earlier code - you successfully loaded the data from the Kaggle dataset and saved it as numpy arrays:\n",
    "- `train_images.npy`\n",
    "- `train_labels.npy`\n",
    "- `test_images.npy`\n",
    "- `test_labels.npy`\n",
    "\n",
    "So the fix is to update the `train_mnist.py` script to load from these `.npy` files instead of trying to load from `mnist.pkl.gz`.\n",
    "\n",
    "Would you like to modify the script to use the numpy files you already have?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a6f49a",
   "metadata": {
    "solveit_ai": true
   },
   "source": [
    "ok give modified version\n",
    "\n",
    "##### ðŸ¤–ReplyðŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "```python\n",
    "%%vms\n",
    "\n",
    "# First, let's clear the old persistent file\n",
    "rm -f train_mnist.py\n",
    "```\n",
    "\n",
    "Then run this updated version:\n",
    "\n",
    "```python\n",
    "%%vms python:tinygrad_env persistent train_mnist.py\n",
    "\n",
    "import numpy as np\n",
    "from tinygrad.tensor import Tensor\n",
    "from tinygrad.nn import Linear\n",
    "from tinygrad.nn.optim import SGD\n",
    "import time\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MNIST Training with Tinygrad on Intel GPU\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load data from numpy files\n",
    "print(\"\\nLoading data from numpy files...\")\n",
    "X_train = np.load('train_images.npy')\n",
    "y_train = np.load('train_labels.npy')\n",
    "X_test = np.load('test_images.npy')\n",
    "y_test = np.load('test_labels.npy')\n",
    "\n",
    "# Flatten images and normalize\n",
    "X_train = X_train.reshape(-1, 784).astype(np.float32) / 255.0\n",
    "X_test = X_test.reshape(-1, 784).astype(np.float32) / 255.0\n",
    "\n",
    "# Create validation split\n",
    "n_val = 10000\n",
    "X_val, y_val = X_train[-n_val:], y_train[-n_val:]\n",
    "X_train, y_train = X_train[:-n_val], y_train[:-n_val]\n",
    "\n",
    "# Use smaller subset for quick demo\n",
    "n_samples = 5000\n",
    "X_train, y_train = X_train[:n_samples], y_train[:n_samples]\n",
    "X_val, y_val = X_val[:1000], y_val[:1000]\n",
    "\n",
    "print(f\"\\nDataset loaded:\")\n",
    "print(f\"  Training: {len(X_train)} samples\")\n",
    "print(f\"  Validation: {len(X_val)} samples\")\n",
    "print(f\"  Test: {len(X_test)} samples\")\n",
    "\n",
    "# Simple 2-layer neural network\n",
    "class SimpleNet:\n",
    "    def __init__(self):\n",
    "        self.l1 = Linear(784, 128)\n",
    "        self.l2 = Linear(128, 10)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        x = self.l1(x).relu()\n",
    "        return self.l2(x)\n",
    "\n",
    "model = SimpleNet()\n",
    "opt = SGD([model.l1.weight, model.l1.bias, model.l2.weight, model.l2.bias], lr=0.01)\n",
    "\n",
    "print(f\"\\nModel architecture:\")\n",
    "print(f\"  Input: 784 -> Hidden: 128 -> Output: 10\")\n",
    "print(f\"  Optimizer: SGD (lr=0.01)\")\n",
    "\n",
    "# Training loop\n",
    "batch_size = 128\n",
    "epochs = 5\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Starting training...\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_start = time.time()\n",
    "    epoch_loss = 0\n",
    "    n_batches = 0\n",
    "    \n",
    "    # Shuffle data\n",
    "    indices = np.random.permutation(len(X_train))\n",
    "    Tensor.training = True\n",
    "    \n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        batch_idx = indices[i:i+batch_size]\n",
    "        X_batch = Tensor(X_train[batch_idx])\n",
    "        y_batch = y_train[batch_idx]\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = model(X_batch)\n",
    "        \n",
    "        # Compute loss using one-hot encoding\n",
    "        y_onehot = Tensor(np.eye(10)[y_batch])\n",
    "        log_probs = logits.log_softmax(axis=1)\n",
    "        loss = -(y_onehot * log_probs).sum() / len(batch_idx)\n",
    "        \n",
    "        # Backward pass\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        epoch_loss += loss.numpy()\n",
    "        n_batches += 1\n",
    "    \n",
    "    # Validation accuracy\n",
    "    Tensor.training = False\n",
    "    val_logits = model(Tensor(X_val))\n",
    "    val_preds = val_logits.numpy().argmax(axis=1)\n",
    "    val_acc = (val_preds == y_val).mean()\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start\n",
    "    avg_loss = epoch_loss / n_batches\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{epochs} | Loss: {avg_loss:.4f} | Val Acc: {val_acc*100:.2f}% | Time: {epoch_time:.2f}s\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Training complete!\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Final test accuracy\n",
    "Tensor.training = False\n",
    "test_logits = model(Tensor(X_test))\n",
    "test_preds = test_logits.numpy().argmax(axis=1)\n",
    "test_acc = (test_preds == y_test).mean()\n",
    "print(f\"\\nFinal Test Accuracy: {test_acc*100:.2f}%\")\n",
    "```\n",
    "\n",
    "Key changes:\n",
    "1. Removed the `pickle.load()` code that was looking for `mnist.pkl.gz`\n",
    "2. Load directly from your `.npy` files\n",
    "3. Flatten and normalize the images (reshape to 784 pixels, divide by 255)\n",
    "4. Create validation split from the training data\n",
    "5. Fixed the `Tensor.training` flag placement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de27fbf",
   "metadata": {
    "time_run": "9:12:52p"
   },
   "outputs": [],
   "source": [
    "%%vms\n",
    "\n",
    "# First, let's clear the old persistent file\n",
    "rm -f train_mnist.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fe9ca7",
   "metadata": {
    "time_run": "9:13:04p"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDERR: Traceback (most recent call last):\n",
      "  File \"/home/solvituser/train_mnist.py\", line 88, in <module>\n",
      "    opt.step()\n",
      "  File \"/home/solvituser/tinygrad_env/lib/python3.12/site-packages/tinygrad/nn/optim.py\", line 41, in step\n",
      "    Tensor.realize(*self.schedule_step())\n",
      "  File \"/home/solvituser/tinygrad_env/lib/python3.12/site-packages/tinygrad/tensor.py\", line 4432, in _wrapper\n",
      "    ret = fn(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/solvituser/tinygrad_env/lib/python3.12/site-packages/tinygrad/tensor.py\", line 266, in realize\n",
      "    run_schedule(*self.schedule_with_vars(*lst), do_update_stats=do_update_stats)\n",
      "  File \"/home/solvituser/tinygrad_env/lib/python3.12/site-packages/tinygrad/engine/realize.py\", line 195, in run_schedule\n",
      "    for si, ei in lower_schedule(schedule):\n",
      "  File \"/home/solvituser/tinygrad_env/lib/python3.12/site-packages/tinygrad/engine/realize.py\", line 188, in lower_schedule\n",
      "    raise e\n",
      "  File \"/home/solvituser/tinygrad_env/lib/python3.12/site-packages/tinygrad/engine/realize.py\", line 182, in lower_schedule\n",
      "    try: yield (si, lower_schedule_item(si))\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/solvituser/tinygrad_env/lib/python3.12/site-packages/tinygrad/engine/realize.py\", line 177, in lower_schedule_item\n",
      "    return ExecItem(*cast(tuple[Runner,list], si_lowerer.rewrite(si.ast, si.bufs)), si.metadata, si.fixedvars)\n",
      "                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/solvituser/tinygrad_env/lib/python3.12/site-packages/tinygrad/uop/ops.py\", line 769, in rewrite\n",
      "    if (ret:=match(uop, ctx)) is not None and ret is not uop: return ret\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"<string>\", line 3, in compiled_match\n",
      "  File \"/home/solvituser/tinygrad_env/lib/python3.12/site-packages/tinygrad/engine/realize.py\", line 170, in <lambda>\n",
      "    (UPat(Ops.SINK, name=\"sink\"), lambda ctx,sink: (runner:=get_runner(ctx[0].device, sink), [ctx[x] for x in runner.p.globals])),\n",
      "                                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/solvituser/tinygrad_env/lib/python3.12/site-packages/tinygrad/engine/realize.py\", line 138, in get_runner\n",
      "    method_cache[ckey] = method_cache[bkey] = ret = CompiledRunner(replace(prg, device=device))\n",
      "                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/solvituser/tinygrad_env/lib/python3.12/site-packages/tinygrad/engine/realize.py\", line 69, in __init__\n",
      "    self.lib = Device[p.device].compiler.compile_cached(p.src)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/solvituser/tinygrad_env/lib/python3.12/site-packages/tinygrad/device.py\", line 271, in compile_cached\n",
      "    lib = self.compile(src)\n",
      "          ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/solvituser/tinygrad_env/lib/python3.12/site-packages/tinygrad/runtime/ops_gpu.py\", line 27, in compile\n",
      "    raise CompileError(f\"OpenCL Compile Error\\n\\n{mstr.value.decode()}\")\n",
      "tinygrad.device.CompileError: OpenCL Compile Error\n",
      "\n",
      "29:1:63: error: use of type 'double' requires cl_khr_fp64 support\n",
      "__kernel void r_4_32_10n2(__global float* data0_128, __global double* data1_1280, __global float* data2_128) {\n",
      "                                                              ^\n",
      "29:5:3: error: use of type 'double' requires cl_khr_fp64 support\n",
      "  double val0 = (*(data1_1280+alu0));\n",
      "  ^\n",
      "29:6:3: error: use of type 'double' requires cl_khr_fp64 support\n",
      "  double val1 = (*(data1_1280+(alu0+1)));\n",
      "  ^\n",
      "29:7:3: error: use of type 'double' requires cl_khr_fp64 support\n",
      "  double val2 = (*(data1_1280+(alu0+2)));\n",
      "  ^\n",
      "29:8:3: error: use of type 'double' requires cl_khr_fp64 support\n",
      "  double val3 = (*(data1_1280+(alu0+3)));\n",
      "  ^\n",
      "29:9:3: error: use of type 'double' requires cl_khr_fp64 support\n",
      "  double val4 = (*(data1_1280+(alu0+4)));\n",
      "  ^\n",
      "29:10:3: error: use of type 'double' requires cl_khr_fp64 support\n",
      "  double val5 = (*(data1_1280+(alu0+5)));\n",
      "  ^\n",
      "29:11:3: error: use of type 'double' requires cl_khr_fp64 support\n",
      "  double val6 = (*(data1_1280+(alu0+6)));\n",
      "  ^\n",
      "29:12:3: error: use of type 'double' requires cl_khr_fp64 support\n",
      "  double val7 = (*(data1_1280+(alu0+7)));\n",
      "  ^\n",
      "29:13:3: error: use of type 'double' requires cl_khr_fp64 support\n",
      "  double val8 = (*(data1_1280+(alu0+8)));\n",
      "  ^\n",
      "29:14:3: error: use of type 'double' requires cl_khr_fp64 support\n",
      "  double val9 = (*(data1_1280+(alu0+9)));\n",
      "  ^\n",
      "29:17:42: warning: double precision constant requires cl_khr_fp64, casting to single precision\n",
      "  *(data0_128+alu1) = -((((float)((val0*-0.0078125)))+((float)((val1*-0.0078125)))+((float)((val2*-0.0078125)))+((float)((val3*-0.0078125)))+((float)((val4*-0.0078125)))+((float)((val5*-0.0078125)))+((float)((val6*-0.0078125)))+((float)((val7*-0.0078125)))+((float)((val8*-0.0078125)))+((float)((val9*-0.0078125))))*(1/val10));\n",
      "                                         ^\n",
      "29:17:71: warning: double precision constant requires cl_khr_fp64, casting to single precision\n",
      "  *(data0_128+alu1) = -((((float)((val0*-0.0078125)))+((float)((val1*-0.0078125)))+((float)((val2*-0.0078125)))+((float)((val3*-0.0078125)))+((float)((val4*-0.0078125)))+((float)((val5*-0.0078125)))+((float)((val6*-0.0078125)))+((float)((val7*-0.0078125)))+((float)((val8*-0.0078125)))+((float)((val9*-0.0078125))))*(1/val10));\n",
      "                                                                      ^\n",
      "29:17:100: warning: double precision constant requires cl_khr_fp64, casting to single precision\n",
      "  *(data0_128+alu1) = -((((float)((val0*-0.0078125)))+((float)((val1*-0.0078125)))+((float)((val2*-0.0078125)))+((float)((val3*-0.0078125)))+((float)((val4*-0.0078125)))+((float)((val5*-0.0078125)))+((float)((val6*-0.0078125)))+((float)((val7*-0.0078125)))+((float)((val8*-0.0078125)))+((float)((val9*-0.0078125))))*(1/val10));\n",
      "                                                                                                   ^\n",
      "29:17:129: warning: double precision constant requires cl_khr_fp64, casting to single precision\n",
      "  *(data0_128+alu1) = -((((float)((val0*-0.0078125)))+((float)((val1*-0.0078125)))+((float)((val2*-0.0078125)))+((float)((val3*-0.0078125)))+((float)((val4*-0.0078125)))+((float)((val5*-0.0078125)))+((float)((val6*-0.0078125)))+((float)((val7*-0.0078125)))+((float)((val8*-0.0078125)))+((float)((val9*-0.0078125))))*(1/val10));\n",
      "                                                                                                                                ^\n",
      "29:17:158: warning: double precision constant requires cl_khr_fp64, casting to single precision\n",
      "  *(data0_128+alu1) = -((((float)((val0*-0.0078125)))+((float)((val1*-0.0078125)))+((float)((val2*-0.0078125)))+((float)((val3*-0.0078125)))+((float)((val4*-0.0078125)))+((float)((val5*-0.0078125)))+((float)((val6*-0.0078125)))+((float)((val7*-0.0078125)))+((float)((val8*-0.0078125)))+((float)((val9*-0.0078125))))*(1/val10));\n",
      "                                                                                                                                                             ^\n",
      "29:17:187: warning: double precision constant requires cl_khr_fp64, casting to single precision\n",
      "  *(data0_128+alu1) = -((((float)((val0*-0.0078125)))+((float)((val1*-0.0078125)))+((float)((val2*-0.0078125)))+((float)((val3*-0.0078125)))+((float)((val4*-0.0078125)))+((float)((val5*-0.0078125)))+((float)((val6*-0.0078125)))+((float)((val7*-0.0078125)))+((float)((val8*-0.0078125)))+((float)((val9*-0.0078125))))*(1/val10));\n",
      "                                                                                                                                                                                          ^\n",
      "29:17:216: warning: double precision constant requires cl_khr_fp64, casting to single precision\n",
      "  *(data0_128+alu1) = -((((float)((val0*-0.0078125)))+((float)((val1*-0.0078125)))+((float)((val2*-0.0078125)))+((float)((val3*-0.0078125)))+((float)((val4*-0.0078125)))+((float)((val5*-0.0078125)))+((float)((val6*-0.0078125)))+((float)((val7*-0.0078125)))+((float)((val8*-0.0078125)))+((float)((val9*-0.0078125))))*(1/val10));\n",
      "                                                                                                                                                                                                                       ^\n",
      "29:17:245: warning: double precision constant requires cl_khr_fp64, casting to single precision\n",
      "  *(data0_128+alu1) = -((((float)((val0*-0.0078125)))+((float)((val1*-0.0078125)))+((float)((val2*-0.0078125)))+((float)((val3*-0.0078125)))+((float)((val4*-0.0078125)))+((float)((val5*-0.0078125)))+((float)((val6*-0.0078125)))+((float)((val7*-0.0078125)))+((float)((val8*-0.0078125)))+((float)((val9*-0.0078125))))*(1/val10));\n",
      "                                                                                                                                                                                                                                                    ^\n",
      "29:17:274: warning: double precision constant requires cl_khr_fp64, casting to single precision\n",
      "  *(data0_128+alu1) = -((((float)((val0*-0.0078125)))+((float)((val1*-0.0078125)))+((float)((val2*-0.0078125)))+((float)((val3*-0.0078125)))+((float)((val4*-0.0078125)))+((float)((val5*-0.0078125)))+((float)((val6*-0.0078125)))+((float)((val7*-0.0078125)))+((float)((val8*-0.0078125)))+((float)((val9*-0.0078125))))*(1/val10));\n",
      "                                                                                                                                                                                                                                                                                 ^\n",
      "29:17:303: warning: double precision constant requires cl_khr_fp64, casting to single precision\n",
      "  *(data0_128+alu1) = -((((float)((val0*-0.0078125)))+((float)((val1*-0.0078125)))+((float)((val2*-0.0078125)))+((float)((val3*-0.0078125)))+((float)((val4*-0.0078125)))+((float)((val5*-0.0078125)))+((float)((val6*-0.0078125)))+((float)((val7*-0.0078125)))+((float)((val8*-0.0078125)))+((float)((val9*-0.0078125))))*(1/val10));\n",
      "                                                                                                                                                                                                                                                                                                              ^\n",
      "\n",
      "\n",
      "============================================================\n",
      "MNIST Training with Tinygrad on Intel GPU\n",
      "============================================================\n",
      "\n",
      "Loading data from numpy files...\n",
      "\n",
      "Dataset loaded:\n",
      "  Training: 5000 samples\n",
      "  Validation: 1000 samples\n",
      "  Test: 10000 samples\n",
      "\n",
      "Model architecture:\n",
      "  Input: 784 -> Hidden: 128 -> Output: 10\n",
      "  Optimizer: SGD (lr=0.01)\n",
      "\n",
      "============================================================\n",
      "Starting training...\n",
      "============================================================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%vms python:tinygrad_env persistent train_mnist.py\n",
    "\n",
    "import numpy as np\n",
    "from tinygrad.tensor import Tensor\n",
    "from tinygrad.nn import Linear\n",
    "from tinygrad.nn.optim import SGD\n",
    "import time\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MNIST Training with Tinygrad on Intel GPU\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load data from numpy files\n",
    "print(\"\\nLoading data from numpy files...\")\n",
    "X_train = np.load('train_images.npy')\n",
    "y_train = np.load('train_labels.npy')\n",
    "X_test = np.load('test_images.npy')\n",
    "y_test = np.load('test_labels.npy')\n",
    "\n",
    "# Flatten images and normalize\n",
    "X_train = X_train.reshape(-1, 784).astype(np.float32) / 255.0\n",
    "X_test = X_test.reshape(-1, 784).astype(np.float32) / 255.0\n",
    "\n",
    "# Create validation split\n",
    "n_val = 10000\n",
    "X_val, y_val = X_train[-n_val:], y_train[-n_val:]\n",
    "X_train, y_train = X_train[:-n_val], y_train[:-n_val]\n",
    "\n",
    "# Use smaller subset for quick demo\n",
    "n_samples = 5000\n",
    "X_train, y_train = X_train[:n_samples], y_train[:n_samples]\n",
    "X_val, y_val = X_val[:1000], y_val[:1000]\n",
    "\n",
    "print(f\"\\nDataset loaded:\")\n",
    "print(f\"  Training: {len(X_train)} samples\")\n",
    "print(f\"  Validation: {len(X_val)} samples\")\n",
    "print(f\"  Test: {len(X_test)} samples\")\n",
    "\n",
    "# Simple 2-layer neural network\n",
    "class SimpleNet:\n",
    "    def __init__(self):\n",
    "        self.l1 = Linear(784, 128)\n",
    "        self.l2 = Linear(128, 10)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        x = self.l1(x).relu()\n",
    "        return self.l2(x)\n",
    "\n",
    "model = SimpleNet()\n",
    "opt = SGD([model.l1.weight, model.l1.bias, model.l2.weight, model.l2.bias], lr=0.01)\n",
    "\n",
    "print(f\"\\nModel architecture:\")\n",
    "print(f\"  Input: 784 -> Hidden: 128 -> Output: 10\")\n",
    "print(f\"  Optimizer: SGD (lr=0.01)\")\n",
    "\n",
    "# Training loop\n",
    "batch_size = 128\n",
    "epochs = 5\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Starting training...\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_start = time.time()\n",
    "    epoch_loss = 0\n",
    "    n_batches = 0\n",
    "    \n",
    "    # Shuffle data\n",
    "    indices = np.random.permutation(len(X_train))\n",
    "    Tensor.training = True\n",
    "    \n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        batch_idx = indices[i:i+batch_size]\n",
    "        X_batch = Tensor(X_train[batch_idx])\n",
    "        y_batch = y_train[batch_idx]\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = model(X_batch)\n",
    "        \n",
    "        # Compute loss using one-hot encoding\n",
    "        y_onehot = Tensor(np.eye(10)[y_batch])\n",
    "        log_probs = logits.log_softmax(axis=1)\n",
    "        loss = -(y_onehot * log_probs).sum() / len(batch_idx)\n",
    "        \n",
    "        # Backward pass\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        epoch_loss += loss.numpy()\n",
    "        n_batches += 1\n",
    "    \n",
    "    # Validation accuracy\n",
    "    Tensor.training = False\n",
    "    val_logits = model(Tensor(X_val))\n",
    "    val_preds = val_logits.numpy().argmax(axis=1)\n",
    "    val_acc = (val_preds == y_val).mean()\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start\n",
    "    avg_loss = epoch_loss / n_batches\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{epochs} | Loss: {avg_loss:.4f} | Val Acc: {val_acc*100:.2f}% | Time: {epoch_time:.2f}s\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Training complete!\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Final test accuracy\n",
    "Tensor.training = False\n",
    "test_logits = model(Tensor(X_test))\n",
    "test_preds = test_logits.numpy().argmax(axis=1)\n",
    "test_acc = (test_preds == y_test).mean()\n",
    "print(f\"\\nFinal Test Accuracy: {test_acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff97843",
   "metadata": {
    "solveit_ai": true
   },
   "source": [
    "\n",
    "ok what happened let's go through the error message\n",
    "\n",
    "##### ðŸ¤–ReplyðŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "Looking at the error message carefully:\n",
    "\n",
    "**Main Error:**\n",
    "```\n",
    "tinygrad.device.CompileError: OpenCL Compile Error\n",
    "error: use of type 'double' requires cl_khr_fp64 support\n",
    "```\n",
    "\n",
    "**What's happening:**\n",
    "\n",
    "1. **The Intel GPU doesn't support double precision (float64)** - The OpenCL compiler is trying to use `double` types, but your Intel GPU doesn't have the `cl_khr_fp64` extension enabled/available.\n",
    "\n",
    "2. **Where doubles are coming from** - Look at this line in the error:\n",
    "   ```\n",
    "   __global double* data1_1280\n",
    "   ```\n",
    "   Tinygrad is generating code that uses `double` for one of the buffers.\n",
    "\n",
    "3. **The specific operation** - The error shows it's happening during the optimizer step (`opt.step()`), and the values being used are things like `-0.0078125` which is `1/128` (the learning rate divided by batch size).\n",
    "\n",
    "**The root cause:** When we created the labels, they're probably being stored as `int64` (which numpy uses by default), and somewhere in the computation this is getting converted to `float64` instead of `float32`.\n",
    "\n",
    "**The fix:** We need to ensure ALL data is `float32`, including the labels when we create the one-hot encoding. Let me show you the corrected version:\n",
    "\n",
    "```python\n",
    "%%vms\n",
    "\n",
    "rm -f train_mnist.py\n",
    "```\n",
    "\n",
    "```python\n",
    "%%vms python:tinygrad_env persistent train_mnist.py\n",
    "\n",
    "import numpy as np\n",
    "from tinygrad.tensor import Tensor\n",
    "from tinygrad.nn import Linear\n",
    "from tinygrad.nn.optim import SGD\n",
    "import time\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MNIST Training with Tinygrad on Intel GPU\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load data from numpy files\n",
    "print(\"\\nLoading data from numpy files...\")\n",
    "X_train = np.load('train_images.npy')\n",
    "y_train = np.load('train_labels.npy')\n",
    "X_test = np.load('test_images.npy')\n",
    "y_test = np.load('test_labels.npy')\n",
    "\n",
    "# Flatten images and normalize - ENSURE FLOAT32\n",
    "X_train = X_train.reshape(-1, 784).astype(np.float32) / 255.0\n",
    "X_test = X_test.reshape(-1, 784).astype(np.float32) / 255.0\n",
    "\n",
    "# Create validation split\n",
    "n_val = 10000\n",
    "X_val, y_val = X_train[-n_val:], y_train[-n_val:]\n",
    "X_train, y_train = X_train[:-n_val], y_train[:-n_val]\n",
    "\n",
    "# Use smaller subset for quick demo\n",
    "n_samples = 5000\n",
    "X_train, y_train = X_train[:n_samples], y_train[:n_samples]\n",
    "X_val, y_val = X_val[:1000], y_val[:1000]\n",
    "\n",
    "print(f\"\\nDataset loaded:\")\n",
    "print(f\"  Training: {len(X_train)} samples\")\n",
    "print(f\"  Validation: {len(X_val)} samples\")\n",
    "print(f\"  Test: {len(X_test)} samples\")\n",
    "\n",
    "# Simple 2-layer neural network\n",
    "class SimpleNet:\n",
    "    def __init__(self):\n",
    "        self.l1 = Linear(784, 128)\n",
    "        self.l2 = Linear(128, 10)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        x = self.l1(x).relu()\n",
    "        return self.l2(x)\n",
    "\n",
    "model = SimpleNet()\n",
    "opt = SGD([model.l1.weight, model.l1.bias, model.l2.weight, model.l2.bias], lr=0.01)\n",
    "\n",
    "print(f\"\\nModel architecture:\")\n",
    "print(f\"  Input: 784 -> Hidden: 128 -> Output: 10\")\n",
    "print(f\"  Optimizer: SGD (lr=0.01)\")\n",
    "\n",
    "# Training loop\n",
    "batch_size = 128\n",
    "epochs = 5\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Starting training...\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_start = time.time()\n",
    "    epoch_loss = 0\n",
    "    n_batches = 0\n",
    "    \n",
    "    # Shuffle data\n",
    "    indices = np.random.permutation(len(X_train))\n",
    "    Tensor.training = True\n",
    "    \n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        batch_idx = indices[i:i+batch_size]\n",
    "        X_batch = Tensor(X_train[batch_idx])\n",
    "        y_batch = y_train[batch_idx]\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = model(X_batch)\n",
    "        \n",
    "        # Compute loss - ENSURE ONE-HOT IS FLOAT32\n",
    "        y_onehot = Tensor(np.eye(10, dtype=np.float32)[y_batch])\n",
    "        log_probs = logits.log_softmax(axis=1)\n",
    "        loss = -(y_onehot * log_probs).sum() / len(batch_idx)\n",
    "        \n",
    "        # Backward pass\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        epoch_loss += loss.numpy()\n",
    "        n_batches += 1\n",
    "    \n",
    "    # Validation accuracy\n",
    "    Tensor.training = False\n",
    "    val_logits = model(Tensor(X_val))\n",
    "    val_preds = val_logits.numpy().argmax(axis=1)\n",
    "    val_acc = (val_preds == y_val).mean()\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start\n",
    "    avg_loss = epoch_loss / n_batches\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{epochs} | Loss: {avg_loss:.4f} | Val Acc: {val_acc*100:.2f}% | Time: {epoch_time:.2f}s\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Training complete!\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Final test accuracy\n",
    "Tensor.training = False\n",
    "test_logits = model(Tensor(X_test))\n",
    "test_preds = test_logits.numpy().argmax(axis=1)\n",
    "test_acc = (test_preds == y_test).mean()\n",
    "print(f\"\\nFinal Test Accuracy: {test_acc*100:.2f}%\")\n",
    "```\n",
    "\n",
    "**Key change:** Line 71 now explicitly specifies `dtype=np.float32`:\n",
    "```python\n",
    "y_onehot = Tensor(np.eye(10, dtype=np.float32)[y_batch])\n",
    "```\n",
    "\n",
    "This ensures the one-hot encoded labels are float32, preventing tinygrad from trying to use double precision on your Intel GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4369f0",
   "metadata": {
    "time_run": "9:14:40p"
   },
   "outputs": [],
   "source": [
    "%%vms\n",
    "\n",
    "rm -f train_mnist.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96f82bb",
   "metadata": {
    "time_run": "9:15:01p"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MNIST Training with Tinygrad on Intel GPU\n",
      "============================================================\n",
      "\n",
      "Loading data from numpy files...\n",
      "\n",
      "Dataset loaded:\n",
      "  Training: 5000 samples\n",
      "  Validation: 1000 samples\n",
      "  Test: 10000 samples\n",
      "\n",
      "Model architecture:\n",
      "  Input: 784 -> Hidden: 128 -> Output: 10\n",
      "  Optimizer: SGD (lr=0.01)\n",
      "\n",
      "============================================================\n",
      "Starting training...\n",
      "============================================================\n",
      "\n",
      "Epoch 1/5 | Loss: 2.2564 | Val Acc: 30.50% | Time: 10.43s\n",
      "Epoch 2/5 | Loss: 2.1668 | Val Acc: 51.40% | Time: 1.35s\n",
      "Epoch 3/5 | Loss: 2.0657 | Val Acc: 62.70% | Time: 1.41s\n",
      "Epoch 4/5 | Loss: 1.9440 | Val Acc: 65.00% | Time: 1.29s\n",
      "Epoch 5/5 | Loss: 1.8175 | Val Acc: 69.40% | Time: 1.36s\n",
      "\n",
      "============================================================\n",
      "Training complete!\n",
      "============================================================\n",
      "\n",
      "Final Test Accuracy: 71.37%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%vms python:tinygrad_env persistent train_mnist.py\n",
    "\n",
    "import numpy as np\n",
    "from tinygrad.tensor import Tensor\n",
    "from tinygrad.nn import Linear\n",
    "from tinygrad.nn.optim import SGD\n",
    "import time\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MNIST Training with Tinygrad on Intel GPU\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load data from numpy files\n",
    "print(\"\\nLoading data from numpy files...\")\n",
    "X_train = np.load('train_images.npy')\n",
    "y_train = np.load('train_labels.npy')\n",
    "X_test = np.load('test_images.npy')\n",
    "y_test = np.load('test_labels.npy')\n",
    "\n",
    "# Flatten images and normalize - ENSURE FLOAT32\n",
    "X_train = X_train.reshape(-1, 784).astype(np.float32) / 255.0\n",
    "X_test = X_test.reshape(-1, 784).astype(np.float32) / 255.0\n",
    "\n",
    "# Create validation split\n",
    "n_val = 10000\n",
    "X_val, y_val = X_train[-n_val:], y_train[-n_val:]\n",
    "X_train, y_train = X_train[:-n_val], y_train[:-n_val]\n",
    "\n",
    "# Use smaller subset for quick demo\n",
    "n_samples = 5000\n",
    "X_train, y_train = X_train[:n_samples], y_train[:n_samples]\n",
    "X_val, y_val = X_val[:1000], y_val[:1000]\n",
    "\n",
    "print(f\"\\nDataset loaded:\")\n",
    "print(f\"  Training: {len(X_train)} samples\")\n",
    "print(f\"  Validation: {len(X_val)} samples\")\n",
    "print(f\"  Test: {len(X_test)} samples\")\n",
    "\n",
    "# Simple 2-layer neural network\n",
    "class SimpleNet:\n",
    "    def __init__(self):\n",
    "        self.l1 = Linear(784, 128)\n",
    "        self.l2 = Linear(128, 10)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        x = self.l1(x).relu()\n",
    "        return self.l2(x)\n",
    "\n",
    "model = SimpleNet()\n",
    "opt = SGD([model.l1.weight, model.l1.bias, model.l2.weight, model.l2.bias], lr=0.01)\n",
    "\n",
    "print(f\"\\nModel architecture:\")\n",
    "print(f\"  Input: 784 -> Hidden: 128 -> Output: 10\")\n",
    "print(f\"  Optimizer: SGD (lr=0.01)\")\n",
    "\n",
    "# Training loop\n",
    "batch_size = 128\n",
    "epochs = 5\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Starting training...\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_start = time.time()\n",
    "    epoch_loss = 0\n",
    "    n_batches = 0\n",
    "    \n",
    "    # Shuffle data\n",
    "    indices = np.random.permutation(len(X_train))\n",
    "    Tensor.training = True\n",
    "    \n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        batch_idx = indices[i:i+batch_size]\n",
    "        X_batch = Tensor(X_train[batch_idx])\n",
    "        y_batch = y_train[batch_idx]\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = model(X_batch)\n",
    "        \n",
    "        # Compute loss - ENSURE ONE-HOT IS FLOAT32\n",
    "        y_onehot = Tensor(np.eye(10, dtype=np.float32)[y_batch])\n",
    "        log_probs = logits.log_softmax(axis=1)\n",
    "        loss = -(y_onehot * log_probs).sum() / len(batch_idx)\n",
    "        \n",
    "        # Backward pass\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        epoch_loss += loss.numpy()\n",
    "        n_batches += 1\n",
    "    \n",
    "    # Validation accuracy\n",
    "    Tensor.training = False\n",
    "    val_logits = model(Tensor(X_val))\n",
    "    val_preds = val_logits.numpy().argmax(axis=1)\n",
    "    val_acc = (val_preds == y_val).mean()\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start\n",
    "    avg_loss = epoch_loss / n_batches\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{epochs} | Loss: {avg_loss:.4f} | Val Acc: {val_acc*100:.2f}% | Time: {epoch_time:.2f}s\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Training complete!\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Final test accuracy\n",
    "Tensor.training = False\n",
    "test_logits = model(Tensor(X_test))\n",
    "test_preds = test_logits.numpy().argmax(axis=1)\n",
    "test_acc = (test_preds == y_test).mean()\n",
    "print(f\"\\nFinal Test Accuracy: {test_acc*100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "solveit_dialog_mode": "standard",
  "solveit_ver": 2
 },
 "nbformat": 4,
 "nbformat_minor": 5
}